{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adv_example_Task2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/8BitRobot/ECE188DeepLearning/blob/main/Adv_example_Task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform an Adversarial attack.\n",
        "\n",
        "For the second part of the project we consider a trained model (MobileNet) which is trained on the imagenet dataset. \n",
        "\n",
        "We use an evasion attack called [FGSM](https://neptune.ai/blog/adversarial-attacks-on-neural-networks-exploring-the-fast-gradient-sign-method#:~:text=The%20Fast%20Gradient%20Sign%20Method%20(FGSM)%20combines%20a%20white%20box,model%20into%20making%20wrong%20predictions.) to fool the neural network into making incorrect predictions."
      ],
      "metadata": {
        "id": "QCeeSPjvxWPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages.\n",
        "\n",
        "Import the necessary packages we continue to use Tensorflow and Keras"
      ],
      "metadata": {
        "id": "WG3kVpwZxZZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 8)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "metadata": {
        "id": "yIK1vfHTyVRa"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Pretrained model. \n",
        "\n",
        "We use the [MobileNetV2](https://arxiv.org/abs/1801.04381) model trained on the [Imagenet](https://www.image-net.org/) dataset. "
      ],
      "metadata": {
        "id": "aPQTeaveyXo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = tf.keras.applications.MobileNetV2(include_top=True,\n",
        "                                                     weights='imagenet')\n",
        "pretrained_model.trainable = False\n",
        "\n",
        "# ImageNet labels\n",
        "decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions"
      ],
      "metadata": {
        "id": "0oom8ZONyy_s"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Function for Data Processing\n",
        "\n",
        "\n",
        "Following functions can be used for data processing. Dont worry about these, just use them. "
      ],
      "metadata": {
        "id": "sZNhLW3_y169"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to preprocess the image so that it can be inputted in MobileNetV2\n",
        "def preprocess(image, target_size):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = tf.image.resize(image, (target_size, target_size))\n",
        "  image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
        "  image = image[None, ...]\n",
        "  return image\n",
        "\n",
        "# Helper function to extract labels from probability vector\n",
        "def get_imagenet_label(probs):\n",
        "  return decode_predictions(probs, top=1)[0][0]"
      ],
      "metadata": {
        "id": "QlvPbIlzzX1A"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load an Image. \n",
        "\n",
        "\n",
        "Load any image, we consider an image of a Golden Retriever. "
      ],
      "metadata": {
        "id": "fXJeav6hzZt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_raw = tf.io.read_file('/content/giantpanda.jpg')\n",
        "image = tf.image.decode_image(image_raw)\n",
        "\n",
        "image = preprocess(image)\n",
        "image_probs = pretrained_model.predict(image)"
      ],
      "metadata": {
        "id": "DCBnw4ynz0VQ",
        "outputId": "9ea1cad2-5079-4734-8404-248b15081357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-fd492fd4cb7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/giantpanda.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m\"string\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   \"\"\"\n\u001b[0;32m--> 133\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_io_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m       return read_file_eager_fallback(\n\u001b[0;32m--> 567\u001b[0;31m           filename, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    568\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file_eager_fallback\u001b[0;34m(filename, name, ctx)\u001b[0m\n\u001b[1;32m    588\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m   _result = _execute.execute(b\"ReadFile\", 1, inputs=_inputs_flat,\n\u001b[0;32m--> 590\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m    591\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     _execute.record_gradient(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: /content/giantpanda.jpg; No such file or directory [Op:ReadFile]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(image[0] * 0.5 + 0.5)  # To change [-1, 1] to [0,1]\n",
        "_, image_class, class_confidence = get_imagenet_label(image_probs)\n",
        "plt.title('{} : {:.2f}% Confidence'.format(image_class, class_confidence*100))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "Z1VBsibzz3Rq",
        "outputId": "96d39e4c-b123-4822-d138-91a2f75813fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b9c3977ed8ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# To change [-1, 1] to [0,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_confidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_imagenet_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{} : {:.2f}% Confidence'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_confidence\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not subscriptable"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Adversarial Image. \n",
        "\n",
        "We use the FGSM method to create an adversarial image. Be sure to read about FGSM to understand how the attack works. "
      ],
      "metadata": {
        "id": "PdT1Y9PYz6NS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "def create_adversarial_pattern(input_image, input_label):\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch(input_image)\n",
        "    prediction = pretrained_model(input_image)\n",
        "    loss = loss_object(input_label, prediction)\n",
        "\n",
        "  # Get the gradients of the loss w.r.t to the input image.\n",
        "  gradient = tape.gradient(loss, input_image)\n",
        "  # Get the sign of the gradients to create the perturbation\n",
        "  signed_grad = tf.sign(gradient)\n",
        "  return signed_grad"
      ],
      "metadata": {
        "id": "P9Y5ZAmX0gS3"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the input label of the image.\n",
        "giant_panda_index = 388\n",
        "label = tf.one_hot(giant_panda_index, image_probs.shape[-1])\n",
        "label = tf.reshape(label, (1, image_probs.shape[-1]))\n",
        "\n",
        "perturbations = create_adversarial_pattern(image, label)\n",
        "plt.imshow(perturbations[0] * 0.5 + 0.5);  # To change [-1, 1] to [0,1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "CSgpF3WI0o4r",
        "outputId": "24860052-119f-46bb-ffc1-e6626ec834ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-91b58e799636>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the input label of the image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgiant_panda_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m388\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgiant_panda_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'image_probs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_images(image, correct):\n",
        "  _, label, confidence = get_imagenet_label(pretrained_model.predict(image))\n",
        "  plt.figure()\n",
        "  plt.imshow(image[0]*0.5+0.5)\n",
        "  description = 'Epsilon = {:0.5f}'.format(eps) if eps else 'Input'\n",
        "  plt.title('{} \\n {} : {:.2f}% Confidence'.format(description,\n",
        "                                                   label, confidence*100))\n",
        "  plt.show()\n",
        "  return label != correct"
      ],
      "metadata": {
        "id": "gzdCLhR06shi"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilons = [0, 0.01, 0.05, 0.1, 0.15]\n",
        "descriptions = [('Epsilon = {:0.3f}'.format(eps) if eps else 'Input')\n",
        "                for eps in epsilons]\n",
        "\n",
        "for i, eps in enumerate(epsilons):\n",
        "  adv_x = image + eps*perturbations\n",
        "  adv_x = tf.clip_by_value(adv_x, -1, 1)\n",
        "  if display_images(adv_x, \"giant_panda\"): break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "WY9vS1jh630F",
        "outputId": "1203971a-e582-45f7-b5b5-7ed72daf1288"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6c0de2b20fc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0madv_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mperturbations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0madv_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdisplay_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"giant_panda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'perturbations' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task2: Perform an Analysis to understand the potency of the attack. \n",
        "\n",
        "Your task here is to understand how small a change could change the class output and this is measured by the epsilon value needed to change the class. \n",
        "\n",
        "Your task is as follows:\n",
        "\n",
        "* Pick 10 images each from different classes in imagenet. \n",
        "* Perform a perturbation analysis on each of these images. \n",
        "* In the analysis you are required to report the smallest epsilon value for which you notice a class change. \n",
        "* Make a table for each of the images considered with the minimum epsilon value for the FGSM attack. \n",
        "\n",
        "Write the Code for the above below. You can add the table also below. "
      ],
      "metadata": {
        "id": "yY4BX-Vj8SZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attack_image(i, step_size, model_num):\n",
        "  global eps\n",
        "\n",
        "  image_name = images[i]\n",
        "  image_label_index = image_label_indices[i]\n",
        "  image = tf.image.decode_image(tf.io.read_file(\"/content/task2-images/\" + image_name + \".jpg\"))\n",
        "  image = preprocess(image, model_input_sizes[model_num])\n",
        "\n",
        "  image_probs = pretrained_model.predict(image)\n",
        "\n",
        "  label = tf.one_hot(image_label_indices[i], image_probs.shape[-1])\n",
        "  label = tf.reshape(label, (1, image_probs.shape[-1]))\n",
        "\n",
        "  perturbations = create_adversarial_pattern(image, label)\n",
        "  # plt.imshow(perturbations[0] * 0.5 + 0.5);  # To change [-1, 1] to [0,1]\n",
        "\n",
        "  eps = 0\n",
        "  adv_x = image\n",
        "  adv_x = tf.clip_by_value(adv_x, -1, 1)\n",
        "  prediction = image_name\n",
        "\n",
        "  while prediction == image_name:\n",
        "    eps += step_size\n",
        "    adv_x = image + eps * perturbations\n",
        "    adv_x = tf.clip_by_value(adv_x, -1, 1)\n",
        "    prediction = get_imagenet_label(pretrained_model.predict(adv_x))[1]\n",
        "  min_epsilon[image_name][model_num] = eps\n",
        "\n",
        "  return adv_x\n",
        "\n",
        "def attack_all_images(model_num):\n",
        "  for i in range(10):\n",
        "    current_step_size = 0.01\n",
        "    res = attack_image(i, current_step_size, model_num)\n",
        "    while min_epsilon[images[i]][model_num] == current_step_size and current_step_size > 0.0001:\n",
        "      current_step_size /= 2\n",
        "      res = attack_image(i, current_step_size, model_num)\n",
        "    # display_images(res, images[i])\n",
        "    print(\"{: <12}{:0.5f}\".format(images[i], min_epsilon[images[i]][model_num]))"
      ],
      "metadata": {
        "id": "f7H95ZBFLDic"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = [\"agaric\", \"bee\", \"cannon\", \"flute\", \"jellyfish\", \"maze\", \"peacock\", \"pizza\", \"screw\", \"teddy\"]\n",
        "image_label_indices = [992, 309, 471, 558, 107, 646, 84, 963, 783, 850]\n",
        "\n",
        "min_epsilon = {}\n",
        "\n",
        "for i in images:\n",
        "  min_epsilon[i] = [0 for i in range(6)]\n",
        "\n",
        "model_input_sizes = [224, 299, 224, 480, 331, 224]\n",
        "\n",
        "eps = 0\n",
        "\n",
        "pretrained_model = tf.keras.applications.MobileNetV2(include_top=True, weights='imagenet')\n",
        "pretrained_model.trainable = False\n",
        "attack_all_images(0)"
      ],
      "metadata": {
        "id": "lvXFdhMZm4v3",
        "outputId": "dd664a59-1dde-4413-d545-1ec2dcb62476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "agaric      0.38000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-078073de3126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpretrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMobileNetV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mattack_all_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-79-ec62e23811af>\u001b[0m in \u001b[0;36mattack_all_images\u001b[0;34m(model_num)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mmin_epsilon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_num\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcurrent_step_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcurrent_step_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mcurrent_step_size\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_step_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;31m# display_images(res, images[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{: <12}{:0.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_epsilon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-ec62e23811af>\u001b[0m in \u001b[0;36mattack_image\u001b[0;34m(i, step_size, model_num)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mperturbations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_adversarial_pattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0;31m# plt.imshow(perturbations[0] * 0.5 + 0.5);  # To change [-1, 1] to [0,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-3565dbc4214b>\u001b[0m in \u001b[0;36mcreate_adversarial_pattern\u001b[0;34m(input_image, input_label)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Get the gradients of the loss w.r.t to the input image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0;31m# Get the sign of the gradients to create the perturbation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0msigned_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    584\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m           data_format=data_format),\n\u001b[0m\u001b[1;32m    587\u001b[0m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[1;32m    588\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_input\u001b[0;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m         data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1245\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "agaric      \t0.38000\n",
        "bee         \t0.00063\n",
        "cannon      \t0.02000\n",
        "flute       \t0.00063\n",
        "jellyfish   \t0.00250\n",
        "maze        \t0.07000\n",
        "peacock     \t0.55000\n",
        "pizza       \t0.00500\n",
        "screw       \t0.00500\n",
        "teddy       \t0.35000\n",
        "```\n",
        "Since the images might be necessary in order to generate these same epsilon values, I've copypasted the table here."
      ],
      "metadata": {
        "id": "6VMJ_D7LDjr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task3: Compare the robustness of the considered model with other models. \n",
        "\n",
        "Your task here is to compare how this model (MobileNetV2) compares with other popular object detection models. \n",
        "\n",
        "Your task is as follows:\n",
        "\n",
        "* Consider 5 different models (you can consider various RESNET architectures, any models you find interesting).\n",
        "* Load the pre-trained weights of the model (trained on imagenet). \n",
        "* Perform Task2 on all the considered models. \n",
        "* Add all the results in the table. Hence the final table you have 6 columns for each model and epsilon values for each of the 10 images for all 6 models. \n",
        "\n",
        "\n",
        "What do you observe? Why do you think this is the case? \n",
        "\n",
        "Write the Code for the above below. You can also add the table and answer to the question below. \n"
      ],
      "metadata": {
        "id": "jVfxkmXVsNYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = tf.keras.applications.MobileNetV2(include_top=True, weights='imagenet')\n",
        "pretrained_model.trainable = False\n",
        "attack_all_images(0)\n",
        "\n",
        "pretrained_model = tf.keras.applications.Xception(include_top=True, weights='imagenet')\n",
        "pretrained_model.trainable = False\n",
        "attack_all_images(1)\n",
        "\n",
        "pretrained_model = tf.keras.applications.MobileNet(include_top=True, weights='imagenet')\n",
        "pretrained_model.trainable = False\n",
        "attack_all_images(2)\n",
        "\n",
        "pretrained_model = tf.keras.applications.EfficientNetV2L(include_top=True, weights='imagenet')\n",
        "pretrained_model.trainable = False\n",
        "attack_all_images(3)\n",
        "\n",
        "pretrained_model = tf.keras.applications.NASNetLarge(include_top=True, weights='imagenet')\n",
        "pretrained_model.trainable = False\n",
        "attack_all_images(4)\n",
        "\n",
        "pretrained_model = tf.keras.applications.ResNet152V2(include_top=True, weights='imagenet')\n",
        "pretrained_model.trainable = False\n",
        "attack_all_images(5)\n",
        "\n",
        "print(\"{:<10} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}\".format(\n",
        "    \"label\",\n",
        "    \"MobileV2\",\n",
        "    \"Xception\",\n",
        "    \"Mobile\",\n",
        "    \"EffV2L\",\n",
        "    \"NASLarge\",\n",
        "    \"Res152V2\"\n",
        "))\n",
        "\n",
        "for i in range(10):\n",
        "  print(\"{:<10} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8}\".format(\n",
        "      images[i], \n",
        "      str(round(min_epsilon[images[i]][0], 5)),\n",
        "      str(round(min_epsilon[images[i]][1], 5)),\n",
        "      str(round(min_epsilon[images[i]][2], 5)),\n",
        "      str(round(min_epsilon[images[i]][3], 5)),\n",
        "      str(round(min_epsilon[images[i]][4], 5)),\n",
        "      str(round(min_epsilon[images[i]][5], 5))\n",
        "  ))"
      ],
      "metadata": {
        "id": "8xoZbVhQFqC3",
        "outputId": "cc07b5ae-9092-4f31-87d7-ca0df68aaa04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "agaric      0.38000\n",
            "bee         0.00063\n",
            "cannon      0.02000\n",
            "flute       0.00063\n",
            "jellyfish   0.00250\n",
            "maze        0.07000\n",
            "peacock     0.55000\n",
            "pizza       0.00500\n",
            "screw       0.00500\n",
            "teddy       0.35000\n",
            "agaric      0.51000\n",
            "bee         0.02000\n",
            "cannon      0.38000\n",
            "flute       0.00500\n",
            "jellyfish   0.40000\n",
            "maze        0.27000\n",
            "peacock     0.67000\n",
            "pizza       0.37000\n",
            "screw       0.33000\n",
            "teddy       0.65000\n",
            "agaric      0.00250\n",
            "bee         0.00008\n",
            "cannon      0.01000\n",
            "flute       0.00008\n",
            "jellyfish   0.00250\n",
            "maze        0.00500\n",
            "peacock     0.02000\n",
            "pizza       0.00250\n",
            "screw       0.00250\n",
            "teddy       0.01000\n",
            "agaric      0.00500\n",
            "bee         0.00500\n",
            "cannon      0.01000\n",
            "flute       0.00008\n",
            "jellyfish   0.00008\n",
            "maze        0.00008\n",
            "peacock     0.00008\n",
            "pizza       0.00008\n",
            "screw       0.04000\n",
            "teddy       0.00008\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-large.h5\n",
            "359751680/359748576 [==============================] - 3s 0us/step\n",
            "359759872/359748576 [==============================] - 3s 0us/step\n",
            "agaric      0.02000\n",
            "bee         0.32000\n",
            "cannon      0.93000\n",
            "flute       0.00008\n",
            "jellyfish   0.41000\n",
            "maze        0.41000\n",
            "peacock     0.45000\n",
            "pizza       0.45000\n",
            "screw       0.71000\n",
            "teddy       1.28000\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels.h5\n",
            "242753536/242745792 [==============================] - 3s 0us/step\n",
            "242761728/242745792 [==============================] - 3s 0us/step\n",
            "agaric      0.36000\n",
            "bee         0.01000\n",
            "cannon      0.22000\n",
            "flute       0.01000\n",
            "jellyfish   0.23000\n",
            "maze        0.28000\n",
            "peacock     0.57000\n",
            "pizza       0.06000\n",
            "screw       0.00500\n",
            "teddy       0.59000\n",
            "label      MobileV2 Xception Mobile   EffV2L   NASLarge Res152V2\n",
            "agaric     0.38     0.51     0.0025   0.005    0.02     0.36    \n",
            "bee        0.00063  0.02     8e-05    0.005    0.32     0.01    \n",
            "cannon     0.02     0.38     0.01     0.01     0.93     0.22    \n",
            "flute      0.00063  0.005    8e-05    8e-05    8e-05    0.01    \n",
            "jellyfish  0.0025   0.4      0.0025   8e-05    0.41     0.23    \n",
            "maze       0.07     0.27     0.005    8e-05    0.41     0.28    \n",
            "peacock    0.55     0.67     0.02     8e-05    0.45     0.57    \n",
            "pizza      0.005    0.37     0.0025   8e-05    0.45     0.06    \n",
            "screw      0.005    0.33     0.0025   0.04     0.71     0.005   \n",
            "teddy      0.35     0.65     0.01     8e-05    1.28     0.59    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "label      MobileV2 Xception Mobile   EffV2L   NASLarge Res152V2\n",
        "\n",
        "agaric     0.38     0.51     0.0025   0.005    0.02     0.36    \n",
        "bee        0.00063  0.02     8e-05    0.005    0.32     0.01    \n",
        "cannon     0.02     0.38     0.01     0.01     0.93     0.22    \n",
        "flute      0.00063  0.005    8e-05    8e-05    8e-05    0.01    \n",
        "jellyfish  0.0025   0.4      0.0025   8e-05    0.41     0.23    \n",
        "maze       0.07     0.27     0.005    8e-05    0.41     0.28    \n",
        "peacock    0.55     0.67     0.02     8e-05    0.45     0.57    \n",
        "pizza      0.005    0.37     0.0025   8e-05    0.45     0.06    \n",
        "screw      0.005    0.33     0.0025   0.04     0.71     0.005   \n",
        "teddy      0.35     0.65     0.01     8e-05    1.28     0.59    \n",
        "```\n",
        "\n",
        "First thing I noticed is that the original MobileNet is not particularly great. On the other hand, NASNetLarge is quite impressive at resisting evasive attacks for some images, but for others (like the flute) it got fooled instantly. The flute's classification was actually quite easily flipped across all 6 networks, so that might be a trait of my image rather than the networks; but regardless, even MobileNet somehow outclassed NASNetLarge for that image. Overall, it looks like NASNetLarge was the most robust of these models, followed by Xception; and MobileNet was by far the most susceptible to attacks on average."
      ],
      "metadata": {
        "id": "NhrycG-EGohP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BONUS: Can you provide a better attack?\n",
        "\n",
        "Can you design a better attack that lowers the epsilon required for the images?\n",
        "\n",
        "Task:\n",
        "\n",
        "* Design another attack. \n",
        "* Compare the epsilon values on 10 images. \n",
        "* Does it perform better than the FGSM attack? That is, does it have lower epsilon values?\n",
        "\n",
        "Write the code and provide your answers below. "
      ],
      "metadata": {
        "id": "EqExuEusxjRH"
      }
    }
  ]
}